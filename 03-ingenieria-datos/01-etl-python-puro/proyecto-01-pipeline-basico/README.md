# ğŸ”§ Pipeline ETL BÃ¡sico con Python Puro

## ğŸ¯ Objetivos de Aprendizaje

- âœ… Comprender el proceso **ETL** (Extract-Transform-Load)
- âœ… Implementar pipeline sin frameworks externos
- âœ… ValidaciÃ³n y limpieza de datos
- âœ… Sistema de logging profesional
- âœ… Manejo robusto de errores

## ğŸ“ Nivel

**Intermedio** - Requiere conocimientos de Python y manejo de archivos

## ğŸ“‹ Conceptos Clave

### ETL

- **Extract**: Lectura de datos de mÃºltiples fuentes
- **Transform**: Limpieza, validaciÃ³n y enriquecimiento
- **Load**: Almacenamiento en formato optimizado

### Componentes

- **Logging**: Registro de todas las operaciones
- **ValidaciÃ³n**: Reglas de negocio
- **Limpieza**: NormalizaciÃ³n de datos
- **Enriquecimiento**: Campos calculados

## ğŸš€ Quick Start

```bash
cd src
python etl_pipeline.py
```

El pipeline:

1. Extrae datos del proyecto EDA (transacciones.csv)
2. Valida y transforma datos
3. Guarda en `data/processed/` (CSV y JSON)
4. Genera log en `logs/`

## ğŸ”„ Proceso ETL

### 1. EXTRACT (ExtracciÃ³n)

```python
def extract_csv(self, file_path):
    try:
        with open(file_path, 'r') as f:
            reader = csv.DictReader(f)
            data = list(reader)
        return data
    except Exception as e:
        logger.error(f"Error: {e}")
        return []
```

**Fuentes soportadas**:

- CSV files
- JSON files
- FÃ¡cil extender para APIs, bases de datos, etc.

### 2. TRANSFORM (TransformaciÃ³n)

**ValidaciÃ³n**:

```python
def validate_record(self, record):
    # Verificar campos requeridos
    # Verificar tipos de datos
    # Verificar rangos vÃ¡lidos
    return is_valid, errors
```

**Limpieza**:

```python
def clean_record(self, record):
    # Eliminar espacios en blanco
    # Normalizar formatos
    # Convertir tipos de datos
    return cleaned_record
```

**Enriquecimiento**:

```python
def enrich_record(self, record):
    # AÃ±adir timestamps
    # Calcular campos derivados
    # AÃ±adir metadatos
    return enriched_record
```

### 3. LOAD (Carga)

```python
def load_json(self, output_path, data):
    with open(output_path, 'w') as f:
        json.dump(data, f, indent=2)
```

**Formatos de salida**:

- JSON (human-readable)
- CSV (compatible con Excel)
- Parquet (columnar, optimizado) - fÃ¡cil de aÃ±adir

## ğŸ“Š ConfiguraciÃ³n

El pipeline se configura mediante diccionario:

```python
config = {
    'sources': [
        {'path': 'data/input.csv', 'type': 'csv'},
        {'path': 'data/input.json', 'type': 'json'}
    ],
    'outputs': [
        {'path': 'data/output.json', 'type': 'json'},
        {'path': 'data/output.csv', 'type': 'csv'}
    ],
    'required_fields': ['id', 'fecha', 'total'],
    'numeric_fields': ['cantidad', 'precio', 'total']
}
```

## ğŸ“ Logging

Cada ejecuciÃ³n genera un log completo:

```
2024-12-18 14:30:00 - INFO - ğŸš€ INICIANDO PIPELINE ETL
2024-12-18 14:30:00 - INFO - ğŸ“¥ Extrayendo datos de CSV
2024-12-18 14:30:01 - INFO -    âœ… 50000 registros extraÃ­dos
2024-12-18 14:30:01 - INFO - ğŸ”„ FASE 2: TRANSFORM
2024-12-18 14:30:05 - WARNING - âš ï¸  Registro invÃ¡lido: campo vacÃ­o
2024-12-18 14:30:10 - INFO - ğŸ’¾ Guardando en JSON
2024-12-18 14:30:12 - INFO - âœ¨ PIPELINE COMPLETADO
2024-12-18 14:30:12 - INFO - â±ï¸  DuraciÃ³n: 12.34 segundos
```

## ğŸ’¡ EstadÃ­sticas

El pipeline rastrea:

- Registros extraÃ­dos
- Registros transformados
- Registros cargados
- Errores encontrados
- Tiempo de ejecuciÃ³n

## ğŸ”§ PersonalizaciÃ³n

### AÃ±adir nueva fuente de datos

```python
def extract_api(self, api_url):
    import requests
    response = requests.get(api_url)
    return response.json()
```

### AÃ±adir nueva regla de validaciÃ³n

```python
def validate_record(self, record):
    # Tu regla personalizada
    if record.get('total', 0) < 0:
        return False, ['Total negativo']
    return True, []
```

### AÃ±adir nueva transformaciÃ³n

```python
def enrich_record(self, record):
    # Calcular descuento
    if 'subtotal' in record and 'total' in record:
        record['descuento'] = record['subtotal'] - record['total']
    return record
```

## ğŸ“ˆ Salida Esperada

```
======================================================================
ğŸš€ INICIANDO PIPELINE ETL
======================================================================

======================================================================
ğŸ”„ FASE 1: EXTRACT (ExtracciÃ³n)
======================================================================
ğŸ“¥ Extrayendo datos de CSV: transacciones.csv
   âœ… 50000 registros extraÃ­dos

ğŸ“Š Total registros extraÃ­dos: 50000

======================================================================
ğŸ”„ FASE 2: TRANSFORM (TransformaciÃ³n)
======================================================================

ğŸ“Š Registros vÃ¡lidos: 49000
âŒ Registros invÃ¡lidos: 1000

======================================================================
ğŸ”„ FASE 3: LOAD (Carga)
======================================================================
ğŸ’¾ Guardando en JSON: transacciones_procesadas.json
   âœ… 49000 registros guardados
ğŸ’¾ Guardando en CSV: transacciones_procesadas.csv
   âœ… 49000 registros guardados

======================================================================
âœ¨ PIPELINE COMPLETADO
======================================================================
â±ï¸  DuraciÃ³n: 15.45 segundos
ğŸ“Š EstadÃ­sticas:
   - ExtraÃ­dos: 50000
   - Transformados: 49000
   - Cargados: 98000
   - Errores: 0
```

## ğŸ› Troubleshooting

**No se encuentran los datos**

- Primero ejecuta el generador de datos del proyecto EDA
- O especifica tu propia fuente en config

**Registros invÃ¡lidos**

- Revisa el log para ver quÃ© campos faltan
- Ajusta `required_fields` en config
- Los datos intencionales tienen ~2% de problemas

**Error de permisos**

- Verifica que puedas escribir en `data/processed/`
- Verifica que puedas escribir en `logs/`

## ğŸ“š PrÃ³ximos Pasos

1. **AÃ±adir mÃ¡s transformaciones**:

   - DeduplicaciÃ³n de registros
   - DetecciÃ³n de outliers
   - NormalizaciÃ³n de texto

2. **Conectar a base de datos**:

   - SQLite para prÃ¡ctica local
   - PostgreSQL para producciÃ³n
   - DuckDB para analytics

3. **AÃ±adir scheduling**:

   - Cron jobs en Linux
   - Task Scheduler en Windows
   - Apache Airflow para orquestaciÃ³n

4. **Siguiente proyecto**:
   - ETL con SQLAlchemy
   - Pipeline con validaciÃ³n Pydantic
   - Streaming con Apache Kafka

## ğŸ’» Mejoras Sugeridas

### DeduplicaciÃ³n

```python
def deduplicate(self, data):
    seen = set()
    unique = []
    for record in data:
        key = record['transaccion_id']
        if key not in seen:
            seen.add(key)
            unique.append(record)
    return unique
```

### Retry Logic

```python
def extract_with_retry(self, file_path, max_retries=3):
    for attempt in range(max_retries):
        try:
            return self.extract_csv(file_path)
        except Exception as e:
            if attempt == max_retries - 1:
                raise
            time.sleep(2 ** attempt)
```

### ParallelizaciÃ³n

```python
from concurrent.futures import ThreadPoolExecutor

def transform_parallel(self, data):
    with ThreadPoolExecutor(max_workers=4) as executor:
        results = executor.map(self.transform_record, data)
    return list(results)
```

## ğŸ“Š Benchmarks

Con 50,000 registros en hardware promedio:

- Extract: ~2 segundos
- Transform: ~10 segundos
- Load: ~3 segundos
- **Total**: ~15 segundos

## ğŸ“ Notas

- El pipeline usa solo la biblioteca estÃ¡ndar de Python
- Los datos se procesan en memoria (ajustar para datasets grandes)
- Los logs son rotados automÃ¡ticamente
- Todos los errores se registran para anÃ¡lisis posterior

---

**ğŸ”§ Build robust data pipelines!**
